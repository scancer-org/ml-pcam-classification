{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCAM Dataset preparation (HDF5 to PyTorch)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFletfT7gTKjf/yMkIl0fg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scancer-org/data-eng-preparation/blob/main/PCAM_Dataset_preparation_(HDF5_to_PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyfzjdpA-y85"
      },
      "source": [
        "### TODO:\n",
        "# Load data - this is the major phase here\n",
        "# Sample out of the data - like 100 examples for train, 20 example for test, same distribution of classes\n",
        "# Have some stats on it\n",
        "# Create (in PyTorch) a base model\n",
        "# Train\n",
        "# Eval\n",
        "# Test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELY-zN5F_DT5"
      },
      "source": [
        "# Test labels (from original, which I'll split for train / test)\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from torch.utils import data\n",
        "from os import listdir\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# anyeone interested should have this path configured already, dataset can be downloaded from GitHub / Drive (PCAM)\n",
        "# Batch Size = 128\n",
        "drive_base_path = 'gdrive/MyDrive/pcamv1/'\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WsftmTCxxDd",
        "outputId": "8eb62b16-cf0e-4590-f92e-cb67a5297e00"
      },
      "source": [
        "drive.mount('/content/gdrive/')\n",
        "!ls gdrive/MyDrive/pcamv1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "camelyonpatch_level_2_split_test_meta.csv\n",
            "camelyonpatch_level_2_split_test_x.h5\n",
            "camelyonpatch_level_2_split_test_y.h5\n",
            "camelyonpatch_level_2_split_train_mask.h5\n",
            "camelyonpatch_level_2_split_train_meta.csv\n",
            "camelyonpatch_level_2_split_train_x.h5\n",
            "camelyonpatch_level_2_split_train_y.h5\n",
            "camelyonpatch_level_2_split_valid_meta.csv\n",
            "camelyonpatch_level_2_split_valid_x.h5\n",
            "camelyonpatch_level_2_split_valid_y.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ_xLXgNBl4Z"
      },
      "source": [
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_valid_x.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_test_y.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_train_y.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_valid_y.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_test_x.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_train_mask.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_train_x.h5.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Lbhq2bzg9P",
        "outputId": "7d8fa633-002f-4ea3-8361-f203a91f6d94"
      },
      "source": [
        "# # Let's get the h5 files\n",
        "# h5_file_names = [f for f in listdir(drive_base_path) if '.h5' in f]\n",
        "# h5_file_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['camelyonpatch_level_2_split_train_mask.h5',\n",
              " 'camelyonpatch_level_2_split_test_x.h5',\n",
              " 'camelyonpatch_level_2_split_test_y.h5',\n",
              " 'camelyonpatch_level_2_split_train_y.h5',\n",
              " 'camelyonpatch_level_2_split_valid_x.h5',\n",
              " 'camelyonpatch_level_2_split_valid_y.h5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpf3tuJjCizb"
      },
      "source": [
        "# # Code is taken from here https://stackoverflow.com/questions/53576113/most-efficient-way-to-use-a-large-data-set-for-pytorch\n",
        "\n",
        "# ## TODO: should implement some kind of a loop, that will load each file (maybe each X and then each Y?), and will do the below:\n",
        "# # 1. read hdf5\n",
        "# # 2. make sure we can parse its values\n",
        "# # 3. store them in PyTorch Datasets accordingly\n",
        "# # Picked one file which already exists in my Drive (validation X)\n",
        "\n",
        "# for h5_file_name in ['camelyonpatch_level_2_split_train_mask.h5']:\n",
        "#   print(\"Some processing on:\", h5_file_name)\n",
        "#   hf = h5py.File(drive_base_path + h5_file_name, 'r')\n",
        "#   # We can take its main key (one and only - X)\n",
        "#   group_key = list(hf.keys())[0]\n",
        "#   print(group_key)\n",
        "#   # ds is the dataset now\n",
        "#   ds = hf[group_key]\n",
        "\n",
        "#   # load only one example from ds\n",
        "#   x = ds[0]\n",
        "#   print(x)\n",
        "#   # load a subset, slice (6 examples) \n",
        "#   arr = ds[:6]\n",
        "#   # print(arr)\n",
        "#   hf.close()\n",
        "# # In simple terms, ds can now be used as an iterator which gives images on the fly \n",
        "# # (i.e. it doesn't load anything in memory). This should make the whole run time blazing fast."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuCZHesM8hcg"
      },
      "source": [
        "# class HDF5Dataset(data.Dataset):\n",
        "#     \"\"\"Represents an abstract HDF5 dataset.\n",
        "    \n",
        "#     Input params:\n",
        "#         file_path: Path to the folder containing the dataset (one or multiple HDF5 files).\n",
        "#         recursive: If True, searches for h5 files in subdirectories.\n",
        "#         load_data: If True, loads all the data immediately into RAM. Use this if\n",
        "#             the dataset is fits into memory. Otherwise, leave this at false and \n",
        "#             the data will load lazily.\n",
        "#         data_cache_size: Number of HDF5 files that can be cached in the cache (default=3).\n",
        "#         transform: PyTorch transform to apply to every data instance (default=None).\n",
        "#     \"\"\"\n",
        "#     def __init__(self, file_path, recursive, load_data, data_cache_size=3, transform=None):\n",
        "#         super().__init__()\n",
        "#         self.data_info = []\n",
        "#         self.data_cache = {}\n",
        "#         self.data_cache_size = data_cache_size\n",
        "#         self.transform = transform\n",
        "\n",
        "#         # Search for all h5 files\n",
        "#         p = Path(file_path)\n",
        "#         assert(p.is_dir())\n",
        "#         if recursive:\n",
        "#             files = sorted(p.glob('**/*.h5'))\n",
        "#         else:\n",
        "#             files = sorted(p.glob('*.h5'))\n",
        "#         if len(files) < 1:\n",
        "#             raise RuntimeError('No hdf5 datasets found')\n",
        "\n",
        "#         for h5dataset_fp in files:\n",
        "#             self._add_data_infos(str(h5dataset_fp.resolve()), load_data)\n",
        "            \n",
        "#     def __getitem__(self, index):\n",
        "#         # get data\n",
        "#         x = self.get_data(\"data\", index)\n",
        "#         if self.transform:\n",
        "#             x = self.transform(x)\n",
        "#         else:\n",
        "#             x = torch.from_numpy(x)\n",
        "\n",
        "#         # get label\n",
        "#         y = self.get_data(\"label\", index)\n",
        "#         y = torch.from_numpy(y)\n",
        "#         return (x, y)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.get_data_infos('data'))\n",
        "    \n",
        "#     def _add_data_infos(self, file_path, load_data):\n",
        "#         with h5py.File(file_path, 'r') as h5_file:\n",
        "#             # Walk through all groups, extracting datasets\n",
        "#             for gname, group in h5_file.items():\n",
        "#                 for ds in group[()]:\n",
        "#                 #.items():\n",
        "#                     # if data is not loaded its cache index is -1\n",
        "#                     idx = -1\n",
        "#                     if load_data:\n",
        "#                         # add data to the data cache\n",
        "#                         idx = self._add_to_cache(ds, file_path)\n",
        "                    \n",
        "#                     # type is derived from the name of the dataset; we expect the dataset\n",
        "#                     # name to have a name such as 'data' or 'label' to identify its type\n",
        "#                     # we also store the shape of the data in case we need it\n",
        "#                     self.data_info.append({'file_path': file_path, 'type': gname, 'shape': ds.shape, 'cache_idx': idx})\n",
        "\n",
        "#     def _load_data(self, file_path):\n",
        "#         \"\"\"Load data to the cache given the file\n",
        "#         path and update the cache index in the\n",
        "#         data_info structure.\n",
        "#         \"\"\"\n",
        "#         with h5py.File(file_path) as h5_file:\n",
        "#             for gname, group in h5_file.items():\n",
        "#                 for ds in group[()]:\n",
        "#                     # add data to the data cache and retrieve\n",
        "#                     # the cache index\n",
        "#                     idx = self._add_to_cache(ds, file_path)\n",
        "\n",
        "#                     # find the beginning index of the hdf5 file we are looking for\n",
        "#                     file_idx = next(i for i,v in enumerate(self.data_info) if v['file_path'] == file_path)\n",
        "\n",
        "#                     # the data info should have the same index since we loaded it in the same way\n",
        "#                     self.data_info[file_idx + idx]['cache_idx'] = idx\n",
        "\n",
        "#         # remove an element from data cache if size was exceeded\n",
        "#         if len(self.data_cache) > self.data_cache_size:\n",
        "#             # remove one item from the cache at random\n",
        "#             removal_keys = list(self.data_cache)\n",
        "#             removal_keys.remove(file_path)\n",
        "#             self.data_cache.pop(removal_keys[0])\n",
        "#             # remove invalid cache_idx\n",
        "#             self.data_info = [{'file_path': di['file_path'], 'type': di['type'], 'shape': di['shape'], 'cache_idx': -1} if di['file_path'] == removal_keys[0] else di for di in self.data_info]\n",
        "\n",
        "#     def _add_to_cache(self, data, file_path):\n",
        "#         \"\"\"Adds data to the cache and returns its index. There is one cache\n",
        "#         list for every file_path, containing all datasets in that file.\n",
        "#         \"\"\"\n",
        "#         if file_path not in self.data_cache:\n",
        "#             self.data_cache[file_path] = [data]\n",
        "#         else:\n",
        "#             self.data_cache[file_path].append(data)\n",
        "#         return len(self.data_cache[file_path]) - 1\n",
        "\n",
        "#     def get_data_infos(self, type):\n",
        "#         \"\"\"Get data infos belonging to a certain type of data.\n",
        "#         \"\"\"\n",
        "#         data_info_type = [di for di in self.data_info if di['type'] == type]\n",
        "#         return data_info_type\n",
        "\n",
        "#     def get_data(self, type, i):\n",
        "#         \"\"\"Call this function anytime you want to access a chunk of data from the\n",
        "#             dataset. This will make sure that the data is loaded in case it is\n",
        "#             not part of the data cache.\n",
        "#         \"\"\"\n",
        "#         fp = self.get_data_infos(type)[i]['file_path']\n",
        "#         print(fp)\n",
        "#         if fp not in self.data_cache:\n",
        "#             self._load_data(fp)\n",
        "        \n",
        "#         # get new cache_idx assigned by _load_data_info\n",
        "#         cache_idx = self.get_data_infos(type)[i]['cache_idx']\n",
        "#         return self.data_cache[fp][cache_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5NJCw3Yaz_8"
      },
      "source": [
        "class H5Dataset_From_2_Files_X_Y(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.file_path = path\n",
        "        self.dataset_x = None\n",
        "        self.dataset_y = None\n",
        "        ### Going to read the X part of the dataset - it's a different file\n",
        "        with h5py.File(self.file_path + '_x.h5', 'r') as filex:\n",
        "            self.dataset_x_len = len(filex['x'])\n",
        "\n",
        "        ### Going to read the y part of the dataset - it's a different file\n",
        "        with h5py.File(self.file_path + '_y.h5', 'r') as filey:\n",
        "            self.dataset_y_len = len(filey['y'])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.dataset_x is None:\n",
        "            self.dataset_x = h5py.File(self.file_path +'_x.h5', 'r')['x']\n",
        "        if self.dataset_y is None:\n",
        "            self.dataset_y = h5py.File(self.file_path +'_y.h5', 'r')['y']\n",
        "        return (self.dataset_x[index], self.dataset_y[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        assert self.dataset_x_len == self.dataset_y_len\n",
        "        return self.dataset_x_len"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XBpM_0ziAuM"
      },
      "source": [
        "dataloader_params = {'batch_size': 128, 'shuffle': True, 'num_workers': 2}\n",
        "\n",
        "train_path = drive_base_path + 'camelyonpatch_level_2_split_train'\n",
        "val_path = drive_base_path + 'camelyonpatch_level_2_split_valid'\n",
        "test_path = drive_base_path + 'camelyonpatch_level_2_split_test'\n",
        "\n",
        "test_dataset = H5Dataset_From_2_Files_X_Y(test_path)\n",
        "test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
        "\n",
        "val_dataset = H5Dataset_From_2_Files_X_Y(val_path)\n",
        "val_dataloader = DataLoader(val_dataset, **dataloader_params)\n",
        "\n",
        "train_dataset = H5Dataset_From_2_Files_X_Y(train_path)\n",
        "train_dataloader = DataLoader(train_dataset, **dataloader_params)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPFBxDo7cWvC",
        "outputId": "29ace8e1-cbd6-4302-ed62-47a70834eaa2"
      },
      "source": [
        "for x, y in test_dataloader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)  \n",
        "  break\n",
        "\n",
        "for x, y in val_dataloader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "for x, y in train_dataloader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 96, 96, 3])\n",
            "torch.Size([128, 1, 1, 1])\n",
            "torch.Size([128, 96, 96, 3])\n",
            "torch.Size([128, 1, 1, 1])\n",
            "torch.Size([128, 96, 96, 3])\n",
            "torch.Size([128, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nSLTF1YioFd",
        "outputId": "01af9ce1-8661-464e-99bd-95d666a4ecac"
      },
      "source": [
        "# for x, y in test_dataloader:\n",
        "#   print(x[0][:][:][:].shape)\n",
        "#   break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([96, 96, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tJkfipOkbAp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}