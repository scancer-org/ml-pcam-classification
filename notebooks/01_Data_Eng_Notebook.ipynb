{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCAM Dataset preparation (HDF5 to PyTorch)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2at/BKXG/TBXvkkK2pUhm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scancer-org/data-eng-preparation/blob/main/PCAM_Dataset_preparation_(HDF5_to_PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyfzjdpA-y85"
      },
      "source": [
        "### TODO:\n",
        "# Load data - this is the major phase here\n",
        "# Sample out of the data - like 100 examples for train, 20 example for test, same distribution of classes\n",
        "# Have some stats on it\n",
        "# Create (in PyTorch) a base model\n",
        "# Train\n",
        "# Eval\n",
        "# Test"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELY-zN5F_DT5"
      },
      "source": [
        "# Test labels (from original, which I'll split for train / test)\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from torch.utils import data\n",
        "from os import listdir\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# anyeone interested should have this path configured already, dataset can be downloaded from GitHub / Drive (PCAM)\n",
        "# Batch Size = 128\n",
        "drive_base_path = 'gdrive/MyDrive/pcamv1/'\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WsftmTCxxDd",
        "outputId": "5f2d2406-820c-452d-f58c-871a7b4b2831"
      },
      "source": [
        "drive.mount('/content/gdrive/')\n",
        "!ls gdrive/MyDrive/pcamv1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "camelyonpatch_level_2_split_test_meta.csv\n",
            "camelyonpatch_level_2_split_test_x.h5\n",
            "camelyonpatch_level_2_split_test_y.h5\n",
            "camelyonpatch_level_2_split_train_mask.h5\n",
            "camelyonpatch_level_2_split_train_meta.csv\n",
            "camelyonpatch_level_2_split_train_x.h5\n",
            "camelyonpatch_level_2_split_train_y.h5\n",
            "camelyonpatch_level_2_split_valid_meta.csv\n",
            "camelyonpatch_level_2_split_valid_x.h5\n",
            "camelyonpatch_level_2_split_valid_y.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ_xLXgNBl4Z"
      },
      "source": [
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_valid_x.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_test_y.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_train_y.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_valid_y.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_test_x.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_train_mask.h5.gz\n",
        "# !gzip -d gdrive/MyDrive/pcamv1/camelyonpatch_level_2_split_train_x.h5.gz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5NJCw3Yaz_8"
      },
      "source": [
        "class H5Dataset_From_2_Files_X_Y(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.file_path = path\n",
        "        self.dataset_x = None\n",
        "        self.dataset_y = None\n",
        "        ### Going to read the X part of the dataset - it's a different file\n",
        "        with h5py.File(self.file_path + '_x.h5', 'r') as filex:\n",
        "            self.dataset_x_len = len(filex['x'])\n",
        "\n",
        "        ### Going to read the y part of the dataset - it's a different file\n",
        "        with h5py.File(self.file_path + '_y.h5', 'r') as filey:\n",
        "            self.dataset_y_len = len(filey['y'])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.dataset_x is None:\n",
        "            self.dataset_x = h5py.File(self.file_path +'_x.h5', 'r')['x']\n",
        "        if self.dataset_y is None:\n",
        "            self.dataset_y = h5py.File(self.file_path +'_y.h5', 'r')['y']\n",
        "        return (self.dataset_x[index], self.dataset_y[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        assert self.dataset_x_len == self.dataset_y_len\n",
        "        return self.dataset_x_len"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XBpM_0ziAuM"
      },
      "source": [
        "dataloader_params = {'batch_size': BATCH_SIZE, 'shuffle': True, 'num_workers': 2}\n",
        "\n",
        "train_path = drive_base_path + 'camelyonpatch_level_2_split_train'\n",
        "val_path = drive_base_path + 'camelyonpatch_level_2_split_valid'\n",
        "test_path = drive_base_path + 'camelyonpatch_level_2_split_test'\n",
        "\n",
        "test_dataset = H5Dataset_From_2_Files_X_Y(test_path)\n",
        "test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
        "\n",
        "val_dataset = H5Dataset_From_2_Files_X_Y(val_path)\n",
        "val_dataloader = DataLoader(val_dataset, **dataloader_params)\n",
        "\n",
        "train_dataset = H5Dataset_From_2_Files_X_Y(train_path)\n",
        "train_dataloader = DataLoader(train_dataset, **dataloader_params)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPFBxDo7cWvC",
        "outputId": "36771cf5-2215-4cb2-9e46-d19e6130b762"
      },
      "source": [
        "for x, y in test_dataloader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "for x, y in val_dataloader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "for x, y in train_dataloader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 96, 96, 3])\n",
            "torch.Size([128, 1, 1, 1])\n",
            "torch.Size([128, 96, 96, 3])\n",
            "torch.Size([128, 1, 1, 1])\n",
            "torch.Size([128, 96, 96, 3])\n",
            "torch.Size([128, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}