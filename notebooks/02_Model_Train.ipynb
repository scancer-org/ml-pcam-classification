{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PCAM - Code + Documentation - Data, Model, Evaluation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scancer-org/data-prep-and-model-train/blob/main/Copy_of_PCAM_Code_%2B_Documentation_Data%2C_Model%2C_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAPZZJDshZlq"
      },
      "source": [
        "# [PCAM Classification](https://github.com/basveeling/pcam) Project\n",
        "## FSDL Online Course - Spring 2021\n",
        "## Daniel Hen, Harish Narayanan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyfzjdpA-y85"
      },
      "source": [
        "### TODO:\n",
        "# 1. Test on test dataloader (final results)\n",
        "# 2. Insert transformations to data - done, just validate by printing and showing some images\n",
        "# 4. Study how to save after training in torch-serve-archiver file (.mar file)\n",
        "# 5. Clean + Document code and have a train for at least 50 epochs"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwUiVfpngMhn"
      },
      "source": [
        "### Installing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZEMtZprUODk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c0f02c-2838-4027-cb28-02d976fd0e00"
      },
      "source": [
        "!pip install -qqq wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1MB 2.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 17.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 18.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyVzEZ1kibEV"
      },
      "source": [
        "### Libraries + Functions import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELY-zN5F_DT5"
      },
      "source": [
        "# Test labels (from original, which I'll split for train / test)\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import wandb\n",
        "import os\n",
        "import pandas as pd\n",
        "import PIL.Image\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import time\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive\n",
        "from torch.utils import data\n",
        "from os import listdir\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms, datasets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6OzATp_ioSN"
      },
      "source": [
        "### Weights & Biases parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "1vM8kvD3U0gs",
        "outputId": "37536d9e-249a-4586-a6e6-a97cde7ba86a"
      },
      "source": [
        "wandb.login()\n",
        "wandb.init(project=\"pcam-pytorch-training\")\n",
        "wandb.run.name = \"pcam-pytorch-experiment#-\" + wandb.run.id\n",
        "print(\"Staring experiment: \", wandb.run.name)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhnarayanan\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.26<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">misty-resonance-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/hnarayanan/pcam-pytorch-training\" target=\"_blank\">https://wandb.ai/hnarayanan/pcam-pytorch-training</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/hnarayanan/pcam-pytorch-training/runs/3migfszg\" target=\"_blank\">https://wandb.ai/hnarayanan/pcam-pytorch-training/runs/3migfszg</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210415_053936-3migfszg</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Staring experiment:  pcam-pytorch-experiment#-3migfszg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHqxj97BiwQH"
      },
      "source": [
        "### Google Drive Mounting - for being able to easily read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WsftmTCxxDd",
        "outputId": "b3b65293-07ac-444c-a55b-8081848846a4"
      },
      "source": [
        "drive.mount('/content/gdrive/')\n",
        "!ls gdrive/MyDrive/pcamv1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n",
            "camelyonpatch_level_2_split_test_meta.csv\n",
            "camelyonpatch_level_2_split_test_x.h5\n",
            "camelyonpatch_level_2_split_test_y.h5\n",
            "camelyonpatch_level_2_split_train_mask.h5\n",
            "camelyonpatch_level_2_split_train_meta.csv\n",
            "camelyonpatch_level_2_split_train_x.h5\n",
            "camelyonpatch_level_2_split_train_y.h5\n",
            "camelyonpatch_level_2_split_valid_meta.csv\n",
            "camelyonpatch_level_2_split_valid_x.h5\n",
            "camelyonpatch_level_2_split_valid_y.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh80yDcLlkxJ"
      },
      "source": [
        "### Class H5Dataset:\n",
        "Defines our dataset class in which we will load data from.\n",
        "<br>\n",
        "Also, deals with hdfs file format, which requires a customized reference in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5NJCw3Yaz_8"
      },
      "source": [
        "class H5Dataset(Dataset):\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.file_path = path\n",
        "        self.dataset_x = None\n",
        "        self.dataset_y = None\n",
        "        self.transform = transform\n",
        "        ### Going to read the X part of the dataset - it's a different file\n",
        "        with h5py.File(self.file_path + '_x.h5', 'r') as filex:\n",
        "            self.dataset_x_len = len(filex['x'])\n",
        "\n",
        "        ### Going to read the y part of the dataset - it's a different file\n",
        "        with h5py.File(self.file_path + '_y.h5', 'r') as filey:\n",
        "            self.dataset_y_len = len(filey['y'])\n",
        "\n",
        "    def __len__(self):\n",
        "        assert self.dataset_x_len == self.dataset_y_len # Since we are reading from different sources, validating we are good in terms of size both X, Y\n",
        "        return self.dataset_x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imgs_path = self.file_path + '_x.h5'\n",
        "        labels_path = self.file_path + '_y.h5'\n",
        "\n",
        "        if self.dataset_x is None:\n",
        "            self.dataset_x = h5py.File(imgs_path, 'r')['x']\n",
        "        if self.dataset_y is None:\n",
        "            self.dataset_y = h5py.File(labels_path, 'r')['y']\n",
        "\n",
        "        # get one pair of X, Y and return them, transform if needed\n",
        "        image = self.dataset_x[index]\n",
        "        label = self.dataset_y[index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image, label)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7tMRivNl3nB"
      },
      "source": [
        "### Configurations params, Dataset + Dataloader Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XBpM_0ziAuM"
      },
      "source": [
        "CHECKPOINT_DIR = 'checkpoint'\n",
        "drive_base_path = 'gdrive/MyDrive/pcamv1/'\n",
        "BATCH_SIZE = 16\n",
        "dataloader_params = {'batch_size': BATCH_SIZE, 'shuffle': True, 'num_workers': 2}\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_path = drive_base_path + 'camelyonpatch_level_2_split_train'\n",
        "val_path = drive_base_path + 'camelyonpatch_level_2_split_valid'\n",
        "test_path = drive_base_path + 'camelyonpatch_level_2_split_test'\n",
        "\n",
        "test_dataset = H5Dataset(test_path, transform=test_transforms)\n",
        "test_loader = DataLoader(test_dataset, **dataloader_params)\n",
        "\n",
        "val_dataset = H5Dataset(val_path, transform=test_transforms)\n",
        "dev_loader = DataLoader(val_dataset, **dataloader_params)\n",
        "\n",
        "train_dataset = H5Dataset(train_path, transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, **dataloader_params)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-LrSeurmb7k"
      },
      "source": [
        "### Model Architecture Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1uLntm-UsP8"
      },
      "source": [
        "class CNN_V2(nn.Module):\n",
        "    \"\"\"Implemented by paper: http://cs230.stanford.edu/projects_winter_2019/posters/15813053.pdf\"\"\"\n",
        "    def __init__(self, p = 0.5):\n",
        "        # log dropout parameter\n",
        "        wandb.config.dropout = p\n",
        "        \"\"\"Init method for initializaing the CNN model\"\"\"\n",
        "        super(CNN_V2, self).__init__()\n",
        "        # 1. Convolutional layers\n",
        "        # Single image is in shape: 3x96x96 (CxHxW, H==W), RGB images\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p = p)\n",
        "        \n",
        "        # 2. FC layers to final output\n",
        "        self.fc1 = nn.Linear(in_features = 128*6*6, out_features = 512)\n",
        "        self.fc_bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(in_features = 512, out_features = 256)\n",
        "        self.fc_bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(in_features = 256, out_features = 128)\n",
        "        self.fc_bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc4 = nn.Linear(in_features = 128, out_features = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution Layers, followed by Batch Normalizations, Maxpool, and ReLU\n",
        "        x = self.bn1(self.conv1(x))                      # batch_size x 96 x 96 x 16\n",
        "        x = self.pool(F.relu(x))                         # batch_size x 48 x 48 x 16\n",
        "        x = self.bn2(self.conv2(x))                      # batch_size x 48 x 48 x 32\n",
        "        x = self.pool(F.relu(x))                         # batch_size x 24 x 24 x 32\n",
        "        x = self.bn3(self.conv3(x))                      # batch_size x 24 x 24 x 64\n",
        "        x = self.pool(F.relu(x))                         # batch_size x 12 x 12 x 64\n",
        "        x = self.bn4(self.conv4(x))                      # batch_size x 12 x 12 x 128\n",
        "        x = self.pool(F.relu(x))                         # batch_size x  6 x  6 x 128\n",
        "        # Flatten the output for each image\n",
        "        x = x.reshape(-1, self.num_flat_features(x))        # batch_size x 6*6*128\n",
        "        \n",
        "        # Apply 4 FC Layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc_bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        x = self.fc_bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        x = self.fc_bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQzHUvzLm6WF"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAuksYO0U4Go"
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"This method calculates the sigmoid function\"\"\"\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def training_accuracy(predicted, true, i, acc, tpr, tnr):\n",
        "    \"\"\"Taken from https://www.kaggle.com/krishanudb/cancer-detection-deep-learning-model-using-pytorch\"\"\"\n",
        "    predicted = predicted.cpu() # Taking the predictions, why cpu and not device?\n",
        "    true = true.cpu() # Taking the labels, why cpu and not device?\n",
        "    \n",
        "    predicted = (sigmoid(predicted.data.numpy()) > 0.5) # Using sigmoid above, if prediction > 0.5 it is 1\n",
        "    true = true.data.numpy() # Numpy - can't combine above?\n",
        "    accuracy = np.sum(predicted == true) / true.shape[0] # Accuracy is: (TP + TN)/(TP + TN + FN + FP)\n",
        "    true_positive_rate = np.sum((predicted == 1) * (true == 1)) / np.sum(true == 1) # TPR: TP / (TP + FN) aka Recall\n",
        "    true_negative_rate = np.sum((predicted == 0) * (true == 0)) / np.sum(true == 0) # TNR: TN / (FP + TN)\n",
        "    acc = acc * (i) / (i + 1) + accuracy / (i + 1)\n",
        "    tpr = tpr * (i) / (i + 1) + true_positive_rate / (i + 1)\n",
        "    tnr = tnr * (i) / (i + 1) + true_negative_rate / (i + 1)\n",
        "    return acc, tpr, tnr\n",
        "\n",
        "def dev_accuracy(predicted, target):\n",
        "    \"\"\"Taken from https://www.kaggle.com/krishanudb/cancer-detection-deep-learning-model-using-pytorch\"\"\"\n",
        "    predicted = predicted.cpu()\n",
        "    target = target.cpu()\n",
        "    predicted = (sigmoid(predicted.data.numpy()) > 0.5)\n",
        "    true = target.data.numpy()\n",
        "    accuracy = np.sum(predicted == true) / true.shape[0]\n",
        "    true_positive_rate = np.sum((predicted == 1) * (true == 1)) / np.sum(true == 1)\n",
        "    true_negative_rate = np.sum((predicted == 0) * (true == 0)) / np.sum(true == 0)\n",
        "    return accuracy, true_positive_rate, true_negative_rate\n",
        "\n",
        "# def train_dev_test_split_indices(dataset_size, train_split=0.8, dev_split=0.1, seed=30):\n",
        "#     \"\"\"Given a dataset size, returns the train/dev/test split indices\"\"\"\n",
        "#     indices = list(range(dataset_size))\n",
        "#     train_split_end = int(np.floor(train_split * dataset_size))\n",
        "#     dev_split_end = int(np.floor(dev_split * dataset_size)) + train_split_end\n",
        "#     np.random.seed(seed)\n",
        "#     np.random.shuffle(indices)\n",
        "#     train_indices = indices[:train_split_end]\n",
        "#     dev_indices = indices[train_split_end:dev_split_end]\n",
        "#     test_indices = indices[dev_split_end:]\n",
        "#     return train_indices, dev_indices, test_indices\n",
        "\n",
        "# def train_dev_test_split_indices_from_dataset(dataset, train_split=0.8, dev_split=0.1, seed=30):\n",
        "#     \"\"\"Given a dataset, returns the train/dev/test split indices\"\"\"\n",
        "#     dataset_size = len(dataset)\n",
        "#     indices = list(range(dataset_size))\n",
        "#     train_split_end = int(np.floor(train_split * dataset_size))\n",
        "#     dev_split_end = int(np.floor(dev_split * dataset_size)) + train_split_end\n",
        "#     np.random.seed(seed)\n",
        "#     np.random.shuffle(indices)\n",
        "#     train_indices = indices[:train_split_end]\n",
        "#     dev_indices = indices[train_split_end:dev_split_end]\n",
        "#     test_indices = indices[dev_split_end:]\n",
        "#     return train_indices, dev_indices, test_indices\n",
        "\n",
        "def fetch_state(epoch, model, optimizer, dev_loss_min, dev_acc_max):\n",
        "    \"\"\"Returns the state dictionary for a model and optimizer\"\"\"\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'dev_loss_min': dev_loss_min,\n",
        "        'dev_acc_max': dev_acc_max,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optim_dict': optimizer.state_dict()\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def save_checkpoint(state, is_best = False, checkpoint = CHECKPOINT_DIR):\n",
        "    \"\"\"Taken from CS230 PyTorch Code Examples\"\"\"\n",
        "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
        "    checkpoint + 'best.pth.tar'\n",
        "\n",
        "    Args:\n",
        "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
        "        is_best: (bool) True if it is the best model seen till now\n",
        "        checkpoint: (string) folder where parameters are to be saved\n",
        "    \"\"\"\n",
        "    filepath = os.path.join(checkpoint, 'last_v2.pth.tar')\n",
        "    if (not os.path.exists(checkpoint)):\n",
        "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
        "        os.mkdir(checkpoint)\n",
        "    else:\n",
        "        print(\"Checkpoint Directory exists! \")\n",
        "    torch.save(state, filepath)\n",
        "    if (is_best):\n",
        "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best_v2.pth.tar'))\n",
        "        \n",
        "def load_checkpoint(model, optimizer = None, checkpoint = CHECKPOINT_DIR):\n",
        "    \"\"\"Taken from CS230 PyTorch Code Examples\"\"\"\n",
        "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
        "    optimizer assuming it is present in checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint: (string) filename which needs to be loaded\n",
        "        model: (torch.nn.Module) model for which the parameters are loaded\n",
        "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint):\n",
        "        print(\"File doesn't exist {}\".format(checkpoint))\n",
        "        checkpoint = None\n",
        "        return\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
        "\n",
        "    return checkpoint"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UH8kiJNgb7W"
      },
      "source": [
        "# Load model\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "\n",
        "model = CNN_V2()\n",
        "if (USE_GPU):\n",
        "    model.cuda()\n",
        "\n",
        "# Hyperparameters\n",
        "lr = 5e-4\n",
        "wandb.config.learning_rate = lr\n",
        "\n",
        "# Parameters\n",
        "num_workers = 0\n",
        "total_epochs = 0\n",
        "num_epochs = 1\n",
        "early_stop_limit = 10\n",
        "bad_epoch_count = 0\n",
        "stop = False\n",
        "train_loss_min = np.Inf\n",
        "dev_loss_min = np.Inf\n",
        "dev_acc_max = 0"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbSRb6jyU7h_"
      },
      "source": [
        "# Optimizer + Loss Function\n",
        "#optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "optimizer = optim.SGD(model.parameters(), lr = lr)   # SWATS\n",
        "criterion = nn.BCEWithLogitsLoss() # Binary Cross Entropy for binary classification - malignant / benign\n",
        "\n",
        "# Load best checkpoint\n",
        "best_checkpoint = os.path.join(CHECKPOINT_DIR, 'best_v2.pth.tar');\n",
        "#checkpoint = load_checkpoint(model = model, optimizer = optimizer, checkpoint = best_checkpoint)\n",
        "# checkpoint = load_checkpoint(model = model, optimizer = None, checkpoint = best_checkpoint) # SWATS\n",
        "# total_epochs = None if checkpoint is None else checkpoint['epoch']\n",
        "total_epochs = 0\n",
        "\n",
        "# Initialize arrays for plot\n",
        "train_loss_arr = []\n",
        "train_acc_arr = []\n",
        "train_tpr_arr = []\n",
        "train_tnr_arr = []\n",
        "\n",
        "dev_loss_arr = []\n",
        "dev_acc_arr = []\n",
        "dev_tpr_arr = []\n",
        "dev_tnr_arr = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNfyyWM2VDKs",
        "outputId": "3736e532-875f-4e3e-8f46-077f81b4a60f"
      },
      "source": [
        "# Loop over the dataset multiple times\n",
        "total_num_epochs = total_epochs + num_epochs\n",
        "for epoch in range(num_epochs):\n",
        "    curr_epoch = total_epochs + epoch + 1\n",
        "    # Keep track of training loss\n",
        "    train_loss = []\n",
        "    # Keep track of dev loss\n",
        "    dev_loss = []\n",
        "    # Keep track of accuracy measurements\n",
        "    acc, tpr, tnr = 0.0, 0.0, 0.0\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        if USE_GPU:\n",
        "            data, target = image.cuda(), label.cuda()\n",
        "        else:\n",
        "            data, target = image, label\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        # Update target to be the same dimensions as output\n",
        "        target = target.view(output.shape[0], 1).float()\n",
        "        # Get accuracy measurements\n",
        "        acc, tpr, tnr = training_accuracy(output, target, batch_idx, acc, tpr, tnr)\n",
        "        # Calculate the batch's loss\n",
        "        curr_train_loss = criterion(output, target)\n",
        "        # Update the training loss\n",
        "        train_loss.append(curr_train_loss.item())\n",
        "        # Backward pass\n",
        "        curr_train_loss.backward()\n",
        "        # Perform a single optimization step to update parameters\n",
        "        optimizer.step()\n",
        "        # Print debug info every 64 batches\n",
        "        if (batch_idx) % 64 == 0:\n",
        "            print('Epoch {}/{}; Iter {}/{}; Loss: {:.4f}; Acc: {:.3f}; True Pos: {:.3f}; True Neg: {:.3f}'\n",
        "                   .format(curr_epoch, total_num_epochs, batch_idx + 1, len(train_loader), curr_train_loss.item(), acc, tpr, tnr))\n",
        "            \n",
        "    end_time = time.time()\n",
        "    \n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(dev_loader):\n",
        "            if USE_GPU:\n",
        "                data, target = image.cuda(), label.cuda()\n",
        "            else:\n",
        "                data, target = image, label\n",
        "            # Get predicted output\n",
        "            output = model(data)\n",
        "            # Update target to be the same dimensions as output\n",
        "            target = target.view(output.shape[0], 1).float()\n",
        "            # Get accuracy measurements\n",
        "            dev_acc, dev_tpr, dev_tnr = dev_accuracy(output, target)\n",
        "            # Calculate the batch's loss\n",
        "            curr_dev_loss = criterion(output, target)\n",
        "            # Update the dev loss\n",
        "            dev_loss.append(curr_dev_loss.item())\n",
        "    \n",
        "    # Calculate average loss\n",
        "    avg_train_loss = np.mean(np.array(train_loss))\n",
        "    avg_dev_loss = np.mean(np.array(dev_loss))\n",
        "    \n",
        "    # Update dev loss arrays\n",
        "    dev_loss_arr.append(avg_dev_loss)\n",
        "    dev_acc_arr.append(dev_acc)\n",
        "    dev_tpr_arr.append(dev_tpr)\n",
        "    dev_tnr_arr.append(dev_tnr)\n",
        "\n",
        "    # Update training loss arrays\n",
        "    train_loss_arr.append(avg_train_loss)\n",
        "    train_acc_arr.append(acc)\n",
        "    train_tpr_arr.append(tpr)\n",
        "    train_tnr_arr.append(tnr)\n",
        "\n",
        "    print('Epoch {}/{}; Avg. Train Loss: {:.4f}; Train Acc: {:.3f}; Train TPR: {:.3f}; Train TNR: {:.3f}; Epoch Time: {} mins; \\nAvg. Dev Loss: {:.4f}; Dev Acc: {:.3f}; Dev TPR: {:.3f}; Dev TNR: {:.3f}\\n'\n",
        "        .format(curr_epoch, total_num_epochs, avg_train_loss, acc, tpr, tnr, round((end_time - start_time)/ 60., 2), avg_dev_loss, dev_acc, dev_tpr, dev_tnr))\n",
        "    \n",
        "    wandb.log({'epoch': curr_epoch, 'loss': avg_train_loss, 'accuracy': acc, 'tpr': tpr, 'time_per_epoch_min': round((end_time - start_time)/ 60., 2)})\n",
        "\n",
        "    if avg_dev_loss < dev_loss_min:\n",
        "        print('Dev loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
        "              .format(dev_loss_min, avg_dev_loss))\n",
        "        dev_loss_min = avg_dev_loss\n",
        "        is_best = False\n",
        "        if (dev_acc >= dev_acc_max):\n",
        "            is_best = True\n",
        "            dev_acc_max = dev_acc\n",
        "        state = fetch_state(epoch = curr_epoch, model = model, optimizer = optimizer, \n",
        "                            dev_loss_min = dev_loss_min, \n",
        "                            dev_acc_max = dev_acc_max)\n",
        "        save_checkpoint(state = state, is_best = is_best)\n",
        "        bad_epoch_count = 0\n",
        "    # If dev loss didn't improve, increase bad_epoch_count and stop if\n",
        "    # bad_epoch_count >= early_stop_limit\n",
        "    else:\n",
        "        bad_epoch_count += 1\n",
        "        print('{} epochs of increasing dev loss ({:.6f} --> {:.6f}).'\n",
        "              .format(bad_epoch_count, dev_loss_min, avg_dev_loss))\n",
        "        if (bad_epoch_count >= early_stop_limit):\n",
        "            print('Stopping training')\n",
        "            stop = True\n",
        "\n",
        "    if (stop):\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1; Iter 1/16384; Loss: 0.7109; Acc: 0.562; True Pos: 0.833; True Neg: 0.400\n",
            "Epoch 1/1; Iter 65/16384; Loss: 0.5951; Acc: 0.498; True Pos: 0.543; True Neg: 0.459\n",
            "Epoch 1/1; Iter 129/16384; Loss: 0.6088; Acc: 0.507; True Pos: 0.534; True Neg: 0.482\n",
            "Epoch 1/1; Iter 193/16384; Loss: 0.6760; Acc: 0.528; True Pos: 0.547; True Neg: 0.512\n",
            "Epoch 1/1; Iter 257/16384; Loss: 0.7358; Acc: 0.540; True Pos: 0.552; True Neg: 0.537\n",
            "Epoch 1/1; Iter 321/16384; Loss: 0.6605; Acc: 0.554; True Pos: 0.561; True Neg: 0.557\n",
            "Epoch 1/1; Iter 385/16384; Loss: 0.6456; Acc: 0.569; True Pos: 0.575; True Neg: 0.572\n",
            "Epoch 1/1; Iter 449/16384; Loss: 0.5869; Acc: 0.585; True Pos: 0.589; True Neg: 0.591\n",
            "Epoch 1/1; Iter 513/16384; Loss: 0.5843; Acc: 0.595; True Pos: 0.599; True Neg: 0.603\n",
            "Epoch 1/1; Iter 577/16384; Loss: 0.5941; Acc: 0.604; True Pos: 0.606; True Neg: 0.615\n",
            "Epoch 1/1; Iter 641/16384; Loss: 0.4511; Acc: 0.615; True Pos: 0.617; True Neg: 0.628\n",
            "Epoch 1/1; Iter 705/16384; Loss: 0.5430; Acc: 0.621; True Pos: 0.623; True Neg: 0.634\n",
            "Epoch 1/1; Iter 769/16384; Loss: 0.4369; Acc: 0.629; True Pos: 0.629; True Neg: 0.644\n",
            "Epoch 1/1; Iter 833/16384; Loss: 0.4283; Acc: 0.636; True Pos: 0.636; True Neg: 0.651\n",
            "Epoch 1/1; Iter 897/16384; Loss: 0.6567; Acc: 0.640; True Pos: 0.642; True Neg: 0.655\n",
            "Epoch 1/1; Iter 961/16384; Loss: 0.6120; Acc: 0.644; True Pos: 0.646; True Neg: 0.660\n",
            "Epoch 1/1; Iter 1025/16384; Loss: 0.7573; Acc: 0.647; True Pos: 0.648; True Neg: 0.664\n",
            "Epoch 1/1; Iter 1089/16384; Loss: 0.5509; Acc: 0.651; True Pos: 0.650; True Neg: 0.669\n",
            "Epoch 1/1; Iter 1153/16384; Loss: 0.5399; Acc: 0.658; True Pos: 0.657; True Neg: 0.676\n",
            "Epoch 1/1; Iter 1217/16384; Loss: 0.7045; Acc: 0.660; True Pos: 0.659; True Neg: 0.680\n",
            "Epoch 1/1; Iter 1281/16384; Loss: 0.8119; Acc: 0.664; True Pos: 0.661; True Neg: 0.684\n",
            "Epoch 1/1; Iter 1345/16384; Loss: 0.5343; Acc: 0.666; True Pos: 0.663; True Neg: 0.688\n",
            "Epoch 1/1; Iter 1409/16384; Loss: 0.6956; Acc: 0.669; True Pos: 0.665; True Neg: 0.691\n",
            "Epoch 1/1; Iter 1473/16384; Loss: 0.4578; Acc: 0.671; True Pos: 0.667; True Neg: 0.693\n",
            "Epoch 1/1; Iter 1537/16384; Loss: 0.5458; Acc: 0.674; True Pos: 0.669; True Neg: 0.697\n",
            "Epoch 1/1; Iter 1601/16384; Loss: 0.6262; Acc: 0.676; True Pos: 0.671; True Neg: 0.700\n",
            "Epoch 1/1; Iter 1665/16384; Loss: 0.5413; Acc: 0.678; True Pos: 0.672; True Neg: 0.703\n",
            "Epoch 1/1; Iter 1729/16384; Loss: 0.3076; Acc: 0.679; True Pos: 0.673; True Neg: 0.704\n",
            "Epoch 1/1; Iter 1793/16384; Loss: 0.6056; Acc: 0.680; True Pos: 0.674; True Neg: 0.706\n",
            "Epoch 1/1; Iter 1857/16384; Loss: 0.5050; Acc: 0.683; True Pos: 0.677; True Neg: 0.709\n",
            "Epoch 1/1; Iter 1921/16384; Loss: 0.4494; Acc: 0.684; True Pos: 0.678; True Neg: 0.711\n",
            "Epoch 1/1; Iter 1985/16384; Loss: 0.6712; Acc: 0.686; True Pos: 0.680; True Neg: 0.713\n",
            "Epoch 1/1; Iter 2049/16384; Loss: 0.5463; Acc: 0.688; True Pos: 0.683; True Neg: 0.715\n",
            "Epoch 1/1; Iter 2113/16384; Loss: 0.5507; Acc: 0.690; True Pos: 0.684; True Neg: 0.717\n",
            "Epoch 1/1; Iter 2177/16384; Loss: 0.4213; Acc: 0.692; True Pos: 0.686; True Neg: 0.719\n",
            "Epoch 1/1; Iter 2241/16384; Loss: 0.4875; Acc: 0.693; True Pos: 0.688; True Neg: 0.720\n",
            "Epoch 1/1; Iter 2305/16384; Loss: 0.4716; Acc: 0.694; True Pos: 0.688; True Neg: 0.721\n",
            "Epoch 1/1; Iter 2369/16384; Loss: 0.8538; Acc: 0.696; True Pos: 0.690; True Neg: 0.723\n",
            "Epoch 1/1; Iter 2433/16384; Loss: 0.4063; Acc: 0.697; True Pos: 0.691; True Neg: 0.724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWAC8393OoE5"
      },
      "source": [
        "model.state_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8-4-2iOsFu"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjfG8QF0ZPl9"
      },
      "source": [
        "help(torch.save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXOpnpoBCBYp"
      },
      "source": [
        "# Possibly important, we need to switch the model from\n",
        "# training mode to evaluation mode\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIj2QisPZdRh"
      },
      "source": [
        "# Option 1: The basic one, as a Python pickle. In the TorchServe\n",
        "# documentation, they call this \"eager mode\" models\n",
        "\n",
        "torch.save(model, 'CNNv2_1.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZGCMQ33Z8x7"
      },
      "source": [
        "# Option 2: What the TorchServe documentation refers to as\n",
        "# \"scripted mode\" models\n",
        "\n",
        "scripted = torch.jit.script(model)\n",
        "scripted.save(\"CNNv2_2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPrbwocCanSd"
      },
      "source": [
        "# Option 3: Traced mode, not even referred to in the TorchServe\n",
        "# documentation\n",
        "\n",
        "example_input = torch.rand(1, 3, 96, 96).to('cuda:0')\n",
        "traced_script = torch.jit.trace(model, example_input)\n",
        "traced_script.save(\"CNNv2_3.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPD7Uw_dammN"
      },
      "source": [
        "# Option 4: Eager mode that saves the \"state dictionary\"\n",
        "# representaton of the model\n",
        "\n",
        "state_dict = model.state_dict()\n",
        "torch.save(state_dict, 'CNNv2_4.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}