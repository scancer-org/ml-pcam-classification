{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PCAM - Code + Documentation - Data, Model, Evaluation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM8s1bxrw2PXr5r1+iQ7iUw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scancer-org/ml-pcam-classification/blob/main/notebooks/08_Fix_NaN_issue_in_division_cleaning_code_2021_05_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAPZZJDshZlq"
      },
      "source": [
        "# [PCAM Classification](https://github.com/basveeling/pcam) Project\n",
        "## FSDL Online Course - Spring 2021\n",
        "## Daniel Hen, Harish Narayanan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwUiVfpngMhn"
      },
      "source": [
        "### Installing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZEMtZprUODk"
      },
      "source": [
        "%%capture\n",
        "!pip install -qqq wandb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyVzEZ1kibEV"
      },
      "source": [
        "### Libraries + Functions import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELY-zN5F_DT5"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import wandb\n",
        "import os\n",
        "import pandas as pd\n",
        "import PIL.Image\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import time\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive\n",
        "from torch.utils import data\n",
        "from os import listdir\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Model file in models\n",
        "from models.cnn_model import ModelCNN"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6OzATp_ioSN"
      },
      "source": [
        "### Weights & Biases parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "1vM8kvD3U0gs",
        "outputId": "00c2cbd3-8394-4939-c3d4-5ad4221ab48c"
      },
      "source": [
        "wandb.login()\n",
        "wandb.init(project=\"pcam-pytorch-training\")\n",
        "wandb.run.name = \"pcam-pytorch-experiment#-\" + wandb.run.id\n",
        "print(\"Staring experiment: \", wandb.run.name)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel8hen\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.28<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">dazzling-frog-34</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/daniel8hen/pcam-pytorch-training\" target=\"_blank\">https://wandb.ai/daniel8hen/pcam-pytorch-training</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/daniel8hen/pcam-pytorch-training/runs/35semd88\" target=\"_blank\">https://wandb.ai/daniel8hen/pcam-pytorch-training/runs/35semd88</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210501_152452-35semd88</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Staring experiment:  pcam-pytorch-experiment#-35semd88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHqxj97BiwQH"
      },
      "source": [
        "### Google Drive Mounting - for being able to easily read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WsftmTCxxDd",
        "outputId": "ea5e6ecb-e7c4-48da-ada5-33dceea0306d"
      },
      "source": [
        "drive.mount('/content/gdrive/')\n",
        "!ls gdrive/MyDrive/pcamv1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "camelyonpatch_level_2_split_test_meta.csv\n",
            "camelyonpatch_level_2_split_test_x.h5\n",
            "camelyonpatch_level_2_split_test_y.h5\n",
            "camelyonpatch_level_2_split_train_mask.h5\n",
            "camelyonpatch_level_2_split_train_meta.csv\n",
            "camelyonpatch_level_2_split_train_x.h5\n",
            "camelyonpatch_level_2_split_train_y.h5\n",
            "camelyonpatch_level_2_split_valid_meta.csv\n",
            "camelyonpatch_level_2_split_valid_x.h5\n",
            "camelyonpatch_level_2_split_valid_y.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh80yDcLlkxJ"
      },
      "source": [
        "### Class H5Dataset:\n",
        "Defines our dataset class in which we will load data from.\n",
        "<br>\n",
        "Also, deals with hdfs file format, which requires a customized reference in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5NJCw3Yaz_8"
      },
      "source": [
        "class H5Dataset(Dataset):\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.file_path = path\n",
        "        self.dataset_x = None\n",
        "        self.dataset_y = None\n",
        "        self.transform = transform\n",
        "        ### Reading X part of HDF5\n",
        "        with h5py.File(self.file_path + '_x.h5', 'r') as filex:\n",
        "            self.dataset_x_len = len(filex['x'])\n",
        "\n",
        "        ### Reading Y part of HDF5\n",
        "        with h5py.File(self.file_path + '_y.h5', 'r') as filey:\n",
        "            self.dataset_y_len = len(filey['y'])\n",
        "\n",
        "    def __len__(self):\n",
        "        assert self.dataset_x_len == self.dataset_y_len # Since we are reading from different sources, validating we are good in terms of size both X, Y\n",
        "        return self.dataset_x_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imgs_path = self.file_path + '_x.h5'\n",
        "        labels_path = self.file_path + '_y.h5'\n",
        "\n",
        "        if self.dataset_x is None:\n",
        "            self.dataset_x = h5py.File(imgs_path, 'r')['x']\n",
        "        if self.dataset_y is None:\n",
        "            self.dataset_y = h5py.File(labels_path, 'r')['y']\n",
        "\n",
        "        # get one pair of X, Y and return them, transform if needed\n",
        "        image = self.dataset_x[index]\n",
        "        label = self.dataset_y[index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image, label)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7tMRivNl3nB"
      },
      "source": [
        "### Configurations params, Dataset + Dataloader Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XBpM_0ziAuM"
      },
      "source": [
        "CHECKPOINT_DIR = 'checkpoint'\n",
        "drive_base_path = 'gdrive/MyDrive/pcamv1/'\n",
        "BATCH_SIZE = 16\n",
        "dataloader_params = {'batch_size': BATCH_SIZE, 'num_workers': 2}\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "### For now, I'm using the train_dataset + train_loader only, splitting it to train and validation by 80% - 20%, accordingly\n",
        "train_path = drive_base_path + 'camelyonpatch_level_2_split_train'\n",
        "val_path = drive_base_path + 'camelyonpatch_level_2_split_valid'\n",
        "test_path = drive_base_path + 'camelyonpatch_level_2_split_test'\n",
        "\n",
        "test_dataset = H5Dataset(test_path, transform=test_transforms)\n",
        "test_loader = DataLoader(test_dataset, **dataloader_params)\n",
        "\n",
        "val_dataset = H5Dataset(val_path, transform=test_transforms)\n",
        "dev_loader = DataLoader(val_dataset, **dataloader_params)\n",
        "\n",
        "train_dataset = H5Dataset(train_path, transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, **dataloader_params)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-LrSeurmb7k"
      },
      "source": [
        "### Model Architecture Class Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQzHUvzLm6WF"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAuksYO0U4Go"
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"This method calculates the sigmoid function\"\"\"\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def training_accuracy(predicted, true, i, acc, tpr, tnr):\n",
        "    \"\"\"Taken from https://www.kaggle.com/krishanudb/cancer-detection-deep-learning-model-using-pytorch\"\"\"\n",
        "    predicted = predicted.cpu() # Taking the predictions, why cpu and not device?\n",
        "    true = true.cpu() # Taking the labels, why cpu and not device?\n",
        "    \n",
        "    predicted = (sigmoid(predicted.data.numpy()) > 0.5) # Using sigmoid above, if prediction > 0.5 it is 1\n",
        "    true = true.data.numpy()\n",
        "    accuracy = np.nan_to_num(np.sum(predicted == true) / true.shape[0]) # Accuracy is: (TP + TN)/(TP + TN + FN + FP)\n",
        "    # true_positive_rate = np.sum((predicted == 1) * (true == 1)) / np.sum(true == 1) # TPR: TP / (TP + FN) aka Recall\n",
        "    true_positive_rate = np.nan_to_num(np.sum((predicted == 1) * (true == 1))/np.sum(true == 1))\n",
        "    true_negative_rate = np.nan_to_num(np.sum((predicted == 0) * (true == 0)) / np.sum(true == 0)) # TNR: TN / (FP + TN)\n",
        "    acc = acc * (i) / (i + 1) + accuracy / (i + 1)\n",
        "    tpr = tpr * (i) / (i + 1) + true_positive_rate / (i + 1)\n",
        "    tnr = tnr * (i) / (i + 1) + true_negative_rate / (i + 1)\n",
        "    return acc, tpr, tnr\n",
        "\n",
        "\n",
        "def dev_accuracy(predicted, target):\n",
        "    \"\"\"Taken from https://www.kaggle.com/krishanudb/cancer-detection-deep-learning-model-using-pytorch\"\"\"\n",
        "    predicted = predicted.cpu()\n",
        "    target = target.cpu()\n",
        "    predicted = (sigmoid(predicted.data.numpy()) > 0.5)\n",
        "    true = target.data.numpy()\n",
        "    accuracy = np.nan_to_num(np.sum(predicted == true) / true.shape[0])\n",
        "    true_positive_rate = np.nan_to_num(np.sum((predicted == 1) * (true == 1)) / np.sum(true == 1))\n",
        "    true_negative_rate = np.nan_to_num(np.sum((predicted == 0) * (true == 0)) / np.sum(true == 0))\n",
        "    return accuracy, true_positive_rate, true_negative_rate\n",
        "\n",
        "def fetch_state(epoch, model, optimizer, dev_loss_min, dev_acc_max):\n",
        "    \"\"\"Returns the state dictionary for a model and optimizer\"\"\"\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'dev_loss_min': dev_loss_min,\n",
        "        'dev_acc_max': dev_acc_max,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optim_dict': optimizer.state_dict()\n",
        "    }\n",
        "    return state\n",
        "\n",
        "def save_checkpoint(state, is_best = False, checkpoint = CHECKPOINT_DIR):\n",
        "    \"\"\"Taken from CS230 PyTorch Code Examples\"\"\"\n",
        "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
        "    checkpoint + 'best.pth.tar'\n",
        "\n",
        "    Args:\n",
        "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
        "        is_best: (bool) True if it is the best model seen till now\n",
        "        checkpoint: (string) folder where parameters are to be saved\n",
        "    \"\"\"\n",
        "    filepath = os.path.join(checkpoint, 'last_v2.pth.tar')\n",
        "    if (not os.path.exists(checkpoint)):\n",
        "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
        "        os.mkdir(checkpoint)\n",
        "    else:\n",
        "        print(\"Checkpoint Directory exists! \")\n",
        "    torch.save(state, filepath)\n",
        "    if (is_best):\n",
        "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best_v2.pth.tar'))\n",
        "        \n",
        "def load_checkpoint(model, optimizer = None, checkpoint = CHECKPOINT_DIR):\n",
        "    \"\"\"Taken from CS230 PyTorch Code Examples\"\"\"\n",
        "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
        "    optimizer assuming it is present in checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint: (string) filename which needs to be loaded\n",
        "        model: (torch.nn.Module) model for which the parameters are loaded\n",
        "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint):\n",
        "        print(\"File doesn't exist {}\".format(checkpoint))\n",
        "        checkpoint = None\n",
        "        return\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "def update_scores(loss_arr, acc_arr, tpr_arr, tnr_arr, curr_avg_loss, curr_acc, curr_tpr, curr_tnr):\n",
        "  \"\"\"This method gets scores / metrics arrays and parameters, and updates them accordingly\"\"\"\n",
        "  loss_arr.append(curr_avg_loss)\n",
        "  acc_arr.append(curr_acc)\n",
        "  tpr_arr.append(curr_tpr)\n",
        "  tnr_arr.append(curr_tnr)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsQAICuT6TRK"
      },
      "source": [
        "### Model, Optimizer, Parameters Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UH8kiJNgb7W"
      },
      "source": [
        "# Model Instantiation\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "\n",
        "model = ModelCNN()\n",
        "# model = ModelCNN(n_conv_output_channels=8) - now configurable per API\n",
        "if (USE_GPU):\n",
        "    model.cuda()\n",
        "\n",
        "# Hyperparameters + Log\n",
        "lr = 5e-4\n",
        "wandb.config.learning_rate = lr\n",
        "\n",
        "# Parameters\n",
        "total_epochs = 0\n",
        "num_epochs = 30\n",
        "early_stop_limit = 15\n",
        "bad_epoch_count = 0\n",
        "stop = False\n",
        "train_loss_min = np.Inf\n",
        "dev_loss_min = np.Inf\n",
        "dev_acc_max = 0\n",
        "\n",
        "# Optimizer + Loss Function\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "# optimizer = optim.SGD(model.parameters(), lr = lr)\n",
        "criterion = nn.BCEWithLogitsLoss() # Binary Cross Entropy for binary classification - malignant / benign\n",
        "\n",
        "best_checkpoint = os.path.join(CHECKPOINT_DIR, 'best_v2.pth.tar') # For saving model\n",
        "\n",
        "total_epochs = 0 # Used in training\n",
        "\n",
        "# Initialize arrays for plot\n",
        "train_loss_arr = []\n",
        "train_acc_arr = []\n",
        "train_tpr_arr = []\n",
        "train_tnr_arr = []\n",
        "\n",
        "dev_loss_arr = []\n",
        "dev_acc_arr = []\n",
        "dev_tpr_arr = []\n",
        "dev_tnr_arr = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvuUUN1r6mCz"
      },
      "source": [
        "### Training + Evaluation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNfyyWM2VDKs",
        "outputId": "2fe2f455-074d-4ac9-ace2-4a6b7f569502"
      },
      "source": [
        "# Loop over the dataset multiple times\n",
        "total_num_epochs = total_epochs + num_epochs\n",
        "for epoch in range(num_epochs):\n",
        "    curr_epoch = total_epochs + epoch + 1\n",
        "    # Keep track of training loss\n",
        "    train_loss = []\n",
        "    # Keep track of dev loss\n",
        "    dev_loss = []\n",
        "    # Keep track of accuracy measurements\n",
        "    acc, tpr, tnr = 0.0, 0.0, 0.0\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        if USE_GPU:\n",
        "            data, target = image.cuda(), label.cuda()\n",
        "        else:\n",
        "            data, target = image, label\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        # Update target to be the same dimensions as output\n",
        "        target = target.view(output.shape[0], 1).float()\n",
        "        # Get accuracy measurements\n",
        "        acc, tpr, tnr = training_accuracy(output, target, batch_idx, acc, tpr, tnr)\n",
        "        # Calculate the batch's loss\n",
        "        curr_train_loss = criterion(output, target)\n",
        "        # Update the training loss\n",
        "        train_loss.append(curr_train_loss.item())\n",
        "        # Backward pass\n",
        "        curr_train_loss.backward()\n",
        "        # Perform a single optimization step to update parameters\n",
        "        optimizer.step()\n",
        "        # Print debug info every 64 batches\n",
        "        if (batch_idx) % 64 == 0:\n",
        "            print('Epoch {}/{}; Iter {}/{}; Loss: {:.4f}; Acc: {:.3f}; True Pos: {:.3f}; True Neg: {:.3f}'\n",
        "                   .format(curr_epoch, total_num_epochs, batch_idx + 1, len(train_loader), curr_train_loss.item(), acc, tpr, tnr))\n",
        "            \n",
        "    end_time = time.time()\n",
        "    \n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(dev_loader):\n",
        "            if USE_GPU:\n",
        "                data, target = image.cuda(), label.cuda()\n",
        "            else:\n",
        "                data, target = image, label\n",
        "            # Get predicted output\n",
        "            output = model(data)\n",
        "            # Update target to be the same dimensions as output\n",
        "            target = target.view(output.shape[0], 1).float()\n",
        "            # Get accuracy measurements\n",
        "            dev_acc, dev_tpr, dev_tnr = dev_accuracy(output, target)\n",
        "            # Calculate the batch's loss\n",
        "            curr_dev_loss = criterion(output, target)\n",
        "            # Update the dev loss\n",
        "            dev_loss.append(curr_dev_loss.item())\n",
        "    \n",
        "    # Calculate average loss\n",
        "    avg_train_loss = np.mean(np.array(train_loss))\n",
        "    avg_dev_loss = np.mean(np.array(dev_loss))\n",
        "    \n",
        "    # Update dev loss arrays\n",
        "    update_scores(dev_loss_arr, dev_acc_arr, dev_tpr_arr, dev_tnr_arr, avg_dev_loss, dev_acc, dev_tpr, dev_tnr)\n",
        "\n",
        "    # Update training loss arrays\n",
        "    update_scores(train_loss_arr, train_acc_arr, train_tpr_arr, train_tnr_arr, avg_train_loss, acc, tpr, tnr)\n",
        "\n",
        "    print('Epoch {}/{}; Avg. Train Loss: {:.4f}; Train Acc: {:.3f}; Train TPR: {:.3f}; Train TNR: {:.3f}; Epoch Time: {} mins; \\nAvg. Dev Loss: {:.4f}; Dev Acc: {:.3f}; Dev TPR: {:.3f}; Dev TNR: {:.3f}\\n'\n",
        "        .format(curr_epoch, total_num_epochs, avg_train_loss, acc, tpr, tnr, round((end_time - start_time)/ 60., 2), avg_dev_loss, dev_acc, dev_tpr, dev_tnr))\n",
        "    \n",
        "    wandb.log({'epoch': curr_epoch, 'loss': avg_train_loss, 'accuracy': acc, 'tpr': tpr, 'time_per_epoch_min': round((end_time - start_time)/ 60., 2)})\n",
        "\n",
        "    if avg_dev_loss < dev_loss_min:\n",
        "        print('Dev loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
        "              .format(dev_loss_min, avg_dev_loss))\n",
        "        dev_loss_min = avg_dev_loss\n",
        "        is_best = False\n",
        "        if (dev_acc >= dev_acc_max):\n",
        "            is_best = True\n",
        "            dev_acc_max = dev_acc\n",
        "        state = fetch_state(epoch = curr_epoch, model = model, optimizer = optimizer, \n",
        "                            dev_loss_min = dev_loss_min, \n",
        "                            dev_acc_max = dev_acc_max)\n",
        "        save_checkpoint(state = state, is_best = is_best)\n",
        "        bad_epoch_count = 0\n",
        "    # If dev loss didn't improve, increase bad_epoch_count and stop if\n",
        "    # bad_epoch_count >= early_stop_limit\n",
        "    else:\n",
        "        bad_epoch_count += 1\n",
        "        print('{} epochs of increasing dev loss ({:.6f} --> {:.6f}).'\n",
        "              .format(bad_epoch_count, dev_loss_min, avg_dev_loss))\n",
        "        if (bad_epoch_count >= early_stop_limit):\n",
        "            print('Stopping training')\n",
        "            stop = True\n",
        "\n",
        "    if (stop):\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30; Iter 1/16384; Loss: 0.7571; Acc: 0.312; True Pos: 0.455; True Neg: 0.000\n",
            "Epoch 1/30; Iter 65/16384; Loss: 0.6102; Acc: 0.663; True Pos: 0.768; True Neg: 0.578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30; Iter 129/16384; Loss: 0.6538; Acc: 0.643; True Pos: 0.672; True Neg: 0.583\n",
            "Epoch 1/30; Iter 193/16384; Loss: 0.7185; Acc: 0.605; True Pos: 0.584; True Neg: 0.544\n",
            "Epoch 1/30; Iter 257/16384; Loss: 0.6048; Acc: 0.581; True Pos: 0.544; True Neg: 0.513\n",
            "Epoch 1/30; Iter 321/16384; Loss: 0.7536; Acc: 0.579; True Pos: 0.525; True Neg: 0.523\n",
            "Epoch 1/30; Iter 385/16384; Loss: 0.7389; Acc: 0.579; True Pos: 0.522; True Neg: 0.539\n",
            "Epoch 1/30; Iter 449/16384; Loss: 0.7309; Acc: 0.582; True Pos: 0.523; True Neg: 0.539\n",
            "Epoch 1/30; Iter 513/16384; Loss: 0.6710; Acc: 0.577; True Pos: 0.511; True Neg: 0.537\n",
            "Epoch 1/30; Iter 577/16384; Loss: 0.6740; Acc: 0.573; True Pos: 0.495; True Neg: 0.537\n",
            "Epoch 1/30; Iter 641/16384; Loss: 0.6744; Acc: 0.572; True Pos: 0.488; True Neg: 0.537\n",
            "Epoch 1/30; Iter 705/16384; Loss: 0.7060; Acc: 0.574; True Pos: 0.488; True Neg: 0.536\n",
            "Epoch 1/30; Iter 769/16384; Loss: 0.7601; Acc: 0.573; True Pos: 0.484; True Neg: 0.530\n",
            "Epoch 1/30; Iter 833/16384; Loss: 0.7121; Acc: 0.572; True Pos: 0.482; True Neg: 0.529\n",
            "Epoch 1/30; Iter 897/16384; Loss: 0.7111; Acc: 0.572; True Pos: 0.476; True Neg: 0.528\n",
            "Epoch 1/30; Iter 961/16384; Loss: 0.7257; Acc: 0.570; True Pos: 0.475; True Neg: 0.526\n",
            "Epoch 1/30; Iter 1025/16384; Loss: 0.7746; Acc: 0.571; True Pos: 0.475; True Neg: 0.532\n",
            "Epoch 1/30; Iter 1089/16384; Loss: 0.6272; Acc: 0.570; True Pos: 0.470; True Neg: 0.532\n",
            "Epoch 1/30; Iter 1153/16384; Loss: 0.7889; Acc: 0.571; True Pos: 0.471; True Neg: 0.532\n",
            "Epoch 1/30; Iter 1217/16384; Loss: 0.6822; Acc: 0.572; True Pos: 0.474; True Neg: 0.535\n",
            "Epoch 1/30; Iter 1281/16384; Loss: 0.5771; Acc: 0.572; True Pos: 0.475; True Neg: 0.534\n",
            "Epoch 1/30; Iter 1345/16384; Loss: 0.6395; Acc: 0.573; True Pos: 0.474; True Neg: 0.536\n",
            "Epoch 1/30; Iter 1409/16384; Loss: 0.7444; Acc: 0.574; True Pos: 0.473; True Neg: 0.537\n",
            "Epoch 1/30; Iter 1473/16384; Loss: 0.7092; Acc: 0.574; True Pos: 0.475; True Neg: 0.539\n",
            "Epoch 1/30; Iter 1537/16384; Loss: 0.5980; Acc: 0.576; True Pos: 0.475; True Neg: 0.544\n",
            "Epoch 1/30; Iter 1601/16384; Loss: 0.6885; Acc: 0.575; True Pos: 0.471; True Neg: 0.542\n",
            "Epoch 1/30; Iter 1665/16384; Loss: 0.6750; Acc: 0.575; True Pos: 0.469; True Neg: 0.539\n",
            "Epoch 1/30; Iter 1729/16384; Loss: 0.7441; Acc: 0.576; True Pos: 0.471; True Neg: 0.542\n",
            "Epoch 1/30; Iter 1793/16384; Loss: 0.5836; Acc: 0.577; True Pos: 0.471; True Neg: 0.539\n",
            "Epoch 1/30; Iter 1857/16384; Loss: 0.7487; Acc: 0.578; True Pos: 0.471; True Neg: 0.541\n",
            "Epoch 1/30; Iter 1921/16384; Loss: 0.6536; Acc: 0.578; True Pos: 0.469; True Neg: 0.542\n",
            "Epoch 1/30; Iter 1985/16384; Loss: 0.6484; Acc: 0.580; True Pos: 0.470; True Neg: 0.544\n",
            "Epoch 1/30; Iter 2049/16384; Loss: 0.7079; Acc: 0.579; True Pos: 0.472; True Neg: 0.543\n",
            "Epoch 1/30; Iter 2113/16384; Loss: 0.6978; Acc: 0.579; True Pos: 0.471; True Neg: 0.541\n",
            "Epoch 1/30; Iter 2177/16384; Loss: 0.7804; Acc: 0.579; True Pos: 0.471; True Neg: 0.542\n",
            "Epoch 1/30; Iter 2241/16384; Loss: 0.6803; Acc: 0.579; True Pos: 0.470; True Neg: 0.538\n",
            "Epoch 1/30; Iter 2305/16384; Loss: 0.6904; Acc: 0.579; True Pos: 0.472; True Neg: 0.540\n",
            "Epoch 1/30; Iter 2369/16384; Loss: 0.7491; Acc: 0.579; True Pos: 0.471; True Neg: 0.540\n",
            "Epoch 1/30; Iter 2433/16384; Loss: 0.6398; Acc: 0.579; True Pos: 0.469; True Neg: 0.538\n",
            "Epoch 1/30; Iter 2497/16384; Loss: 0.6836; Acc: 0.578; True Pos: 0.467; True Neg: 0.536\n",
            "Epoch 1/30; Iter 2561/16384; Loss: 0.6390; Acc: 0.579; True Pos: 0.469; True Neg: 0.535\n",
            "Epoch 1/30; Iter 2625/16384; Loss: 0.7615; Acc: 0.579; True Pos: 0.468; True Neg: 0.535\n",
            "Epoch 1/30; Iter 2689/16384; Loss: 0.6213; Acc: 0.580; True Pos: 0.467; True Neg: 0.536\n",
            "Epoch 1/30; Iter 2753/16384; Loss: 0.6441; Acc: 0.580; True Pos: 0.468; True Neg: 0.537\n",
            "Epoch 1/30; Iter 2817/16384; Loss: 0.7632; Acc: 0.581; True Pos: 0.467; True Neg: 0.537\n",
            "Epoch 1/30; Iter 2881/16384; Loss: 0.6593; Acc: 0.581; True Pos: 0.465; True Neg: 0.537\n",
            "Epoch 1/30; Iter 2945/16384; Loss: 0.5993; Acc: 0.582; True Pos: 0.466; True Neg: 0.538\n",
            "Epoch 1/30; Iter 3009/16384; Loss: 0.6206; Acc: 0.582; True Pos: 0.466; True Neg: 0.540\n",
            "Epoch 1/30; Iter 3073/16384; Loss: 0.7184; Acc: 0.583; True Pos: 0.466; True Neg: 0.541\n",
            "Epoch 1/30; Iter 3137/16384; Loss: 0.6344; Acc: 0.583; True Pos: 0.465; True Neg: 0.541\n",
            "Epoch 1/30; Iter 3201/16384; Loss: 0.6324; Acc: 0.585; True Pos: 0.467; True Neg: 0.543\n",
            "Epoch 1/30; Iter 3265/16384; Loss: 0.6914; Acc: 0.585; True Pos: 0.467; True Neg: 0.543\n",
            "Epoch 1/30; Iter 3329/16384; Loss: 0.6613; Acc: 0.586; True Pos: 0.466; True Neg: 0.545\n",
            "Epoch 1/30; Iter 3393/16384; Loss: 0.5036; Acc: 0.588; True Pos: 0.467; True Neg: 0.548\n",
            "Epoch 1/30; Iter 3457/16384; Loss: 0.5283; Acc: 0.589; True Pos: 0.467; True Neg: 0.548\n",
            "Epoch 1/30; Iter 3521/16384; Loss: 0.6451; Acc: 0.590; True Pos: 0.467; True Neg: 0.549\n",
            "Epoch 1/30; Iter 3585/16384; Loss: 0.7058; Acc: 0.591; True Pos: 0.467; True Neg: 0.549\n",
            "Epoch 1/30; Iter 3649/16384; Loss: 0.7936; Acc: 0.591; True Pos: 0.466; True Neg: 0.549\n",
            "Epoch 1/30; Iter 3713/16384; Loss: 0.6474; Acc: 0.591; True Pos: 0.465; True Neg: 0.548\n",
            "Epoch 1/30; Iter 3777/16384; Loss: 0.6825; Acc: 0.592; True Pos: 0.466; True Neg: 0.549\n",
            "Epoch 1/30; Iter 3841/16384; Loss: 0.5452; Acc: 0.593; True Pos: 0.465; True Neg: 0.551\n",
            "Epoch 1/30; Iter 3905/16384; Loss: 0.6279; Acc: 0.594; True Pos: 0.465; True Neg: 0.553\n",
            "Epoch 1/30; Iter 3969/16384; Loss: 0.5103; Acc: 0.595; True Pos: 0.465; True Neg: 0.555\n",
            "Epoch 1/30; Iter 4033/16384; Loss: 0.6430; Acc: 0.597; True Pos: 0.466; True Neg: 0.557\n",
            "Epoch 1/30; Iter 4097/16384; Loss: 0.6284; Acc: 0.598; True Pos: 0.466; True Neg: 0.558\n",
            "Epoch 1/30; Iter 4161/16384; Loss: 0.7660; Acc: 0.599; True Pos: 0.466; True Neg: 0.558\n",
            "Epoch 1/30; Iter 4225/16384; Loss: 0.5718; Acc: 0.601; True Pos: 0.466; True Neg: 0.560\n",
            "Epoch 1/30; Iter 4289/16384; Loss: 0.5637; Acc: 0.601; True Pos: 0.466; True Neg: 0.559\n",
            "Epoch 1/30; Iter 4353/16384; Loss: 0.5353; Acc: 0.602; True Pos: 0.466; True Neg: 0.560\n",
            "Epoch 1/30; Iter 4417/16384; Loss: 0.5465; Acc: 0.604; True Pos: 0.466; True Neg: 0.562\n",
            "Epoch 1/30; Iter 4481/16384; Loss: 0.6142; Acc: 0.606; True Pos: 0.467; True Neg: 0.565\n",
            "Epoch 1/30; Iter 4545/16384; Loss: 0.6407; Acc: 0.606; True Pos: 0.466; True Neg: 0.565\n",
            "Epoch 1/30; Iter 4609/16384; Loss: 0.7479; Acc: 0.607; True Pos: 0.467; True Neg: 0.567\n",
            "Epoch 1/30; Iter 4673/16384; Loss: 0.5170; Acc: 0.608; True Pos: 0.467; True Neg: 0.567\n",
            "Epoch 1/30; Iter 4737/16384; Loss: 0.6305; Acc: 0.609; True Pos: 0.466; True Neg: 0.568\n",
            "Epoch 1/30; Iter 4801/16384; Loss: 0.6205; Acc: 0.610; True Pos: 0.466; True Neg: 0.570\n",
            "Epoch 1/30; Iter 4865/16384; Loss: 0.5041; Acc: 0.611; True Pos: 0.465; True Neg: 0.572\n",
            "Epoch 1/30; Iter 4929/16384; Loss: 0.5072; Acc: 0.612; True Pos: 0.466; True Neg: 0.572\n",
            "Epoch 1/30; Iter 4993/16384; Loss: 0.7230; Acc: 0.613; True Pos: 0.464; True Neg: 0.574\n",
            "Epoch 1/30; Iter 5057/16384; Loss: 0.6106; Acc: 0.614; True Pos: 0.463; True Neg: 0.575\n",
            "Epoch 1/30; Iter 5121/16384; Loss: 0.5827; Acc: 0.615; True Pos: 0.464; True Neg: 0.575\n",
            "Epoch 1/30; Iter 5185/16384; Loss: 0.7153; Acc: 0.616; True Pos: 0.463; True Neg: 0.577\n",
            "Epoch 1/30; Iter 5249/16384; Loss: 0.5413; Acc: 0.617; True Pos: 0.463; True Neg: 0.578\n",
            "Epoch 1/30; Iter 5313/16384; Loss: 0.6925; Acc: 0.618; True Pos: 0.463; True Neg: 0.580\n",
            "Epoch 1/30; Iter 5377/16384; Loss: 0.5931; Acc: 0.620; True Pos: 0.463; True Neg: 0.581\n",
            "Epoch 1/30; Iter 5441/16384; Loss: 0.7488; Acc: 0.621; True Pos: 0.464; True Neg: 0.582\n",
            "Epoch 1/30; Iter 5505/16384; Loss: 0.4965; Acc: 0.622; True Pos: 0.464; True Neg: 0.583\n",
            "Epoch 1/30; Iter 5569/16384; Loss: 0.6357; Acc: 0.624; True Pos: 0.464; True Neg: 0.585\n",
            "Epoch 1/30; Iter 5633/16384; Loss: 0.5101; Acc: 0.625; True Pos: 0.463; True Neg: 0.587\n",
            "Epoch 1/30; Iter 5697/16384; Loss: 0.6531; Acc: 0.626; True Pos: 0.463; True Neg: 0.588\n",
            "Epoch 1/30; Iter 5761/16384; Loss: 0.4527; Acc: 0.627; True Pos: 0.464; True Neg: 0.587\n",
            "Epoch 1/30; Iter 5825/16384; Loss: 0.5174; Acc: 0.628; True Pos: 0.463; True Neg: 0.588\n",
            "Epoch 1/30; Iter 5889/16384; Loss: 0.6566; Acc: 0.630; True Pos: 0.464; True Neg: 0.589\n",
            "Epoch 1/30; Iter 5953/16384; Loss: 0.4422; Acc: 0.630; True Pos: 0.464; True Neg: 0.590\n",
            "Epoch 1/30; Iter 6017/16384; Loss: 0.4770; Acc: 0.631; True Pos: 0.464; True Neg: 0.589\n",
            "Epoch 1/30; Iter 6081/16384; Loss: 0.5506; Acc: 0.632; True Pos: 0.464; True Neg: 0.590\n",
            "Epoch 1/30; Iter 6145/16384; Loss: 0.6046; Acc: 0.633; True Pos: 0.464; True Neg: 0.590\n",
            "Epoch 1/30; Iter 6209/16384; Loss: 0.3573; Acc: 0.635; True Pos: 0.463; True Neg: 0.591\n",
            "Epoch 1/30; Iter 6273/16384; Loss: 0.4213; Acc: 0.636; True Pos: 0.464; True Neg: 0.590\n",
            "Epoch 1/30; Iter 6337/16384; Loss: 0.4865; Acc: 0.637; True Pos: 0.464; True Neg: 0.590\n",
            "Epoch 1/30; Iter 6401/16384; Loss: 0.8476; Acc: 0.639; True Pos: 0.464; True Neg: 0.592\n",
            "Epoch 1/30; Iter 6465/16384; Loss: 0.4631; Acc: 0.640; True Pos: 0.465; True Neg: 0.592\n",
            "Epoch 1/30; Iter 6529/16384; Loss: 0.3268; Acc: 0.641; True Pos: 0.465; True Neg: 0.593\n",
            "Epoch 1/30; Iter 6593/16384; Loss: 0.5313; Acc: 0.643; True Pos: 0.466; True Neg: 0.593\n",
            "Epoch 1/30; Iter 6657/16384; Loss: 0.5849; Acc: 0.644; True Pos: 0.466; True Neg: 0.593\n",
            "Epoch 1/30; Iter 6721/16384; Loss: 0.5629; Acc: 0.645; True Pos: 0.465; True Neg: 0.594\n",
            "Epoch 1/30; Iter 6785/16384; Loss: 0.3416; Acc: 0.647; True Pos: 0.466; True Neg: 0.595\n",
            "Epoch 1/30; Iter 6849/16384; Loss: 0.5740; Acc: 0.648; True Pos: 0.467; True Neg: 0.595\n",
            "Epoch 1/30; Iter 6913/16384; Loss: 0.4337; Acc: 0.649; True Pos: 0.467; True Neg: 0.595\n",
            "Epoch 1/30; Iter 6977/16384; Loss: 0.4666; Acc: 0.650; True Pos: 0.466; True Neg: 0.596\n",
            "Epoch 1/30; Iter 7041/16384; Loss: 0.3053; Acc: 0.652; True Pos: 0.467; True Neg: 0.596\n",
            "Epoch 1/30; Iter 7105/16384; Loss: 0.5427; Acc: 0.653; True Pos: 0.468; True Neg: 0.595\n",
            "Epoch 1/30; Iter 7169/16384; Loss: 0.3168; Acc: 0.654; True Pos: 0.467; True Neg: 0.596\n",
            "Epoch 1/30; Iter 7233/16384; Loss: 1.5197; Acc: 0.656; True Pos: 0.467; True Neg: 0.596\n",
            "Epoch 1/30; Iter 7297/16384; Loss: 0.1980; Acc: 0.657; True Pos: 0.468; True Neg: 0.595\n",
            "Epoch 1/30; Iter 7361/16384; Loss: 0.4010; Acc: 0.658; True Pos: 0.468; True Neg: 0.595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpD-HU2v6rLR"
      },
      "source": [
        "### Model Saving + Loading (fot future use, API)\n",
        "This should be sent to Server Side (which hosts it via torchserve)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugvo-7-UCZ0F"
      },
      "source": [
        "# generate a dummy input\n",
        "example_input = torch.rand(1, 3, 96, 96).to(torch.device(\"cuda\"))\n",
        "\n",
        "# Store the existing model using torch.jit\n",
        "traced_script_module = torch.jit.trace(model, example_input)\n",
        "\n",
        "full_filename = \"pcam_cnn_v1.2.pt\"\n",
        "# Save the script module under pcam_cnn.pt\n",
        "traced_script_module.save(full_filename)\n",
        "\n",
        "new_model = torch.jit.load(full_filename)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NTTmCitK5u7",
        "outputId": "40f39b44-e8d5-4d0b-964c-090d0bf73692"
      },
      "source": [
        "!pip install torchserve torch-model-archiver"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchserve\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/b9/eb493d7eb27e2728a4ba48627d6b4f1ded492c96698666d53240f43fb50d/torchserve-0.3.1-py2.py3-none-any.whl (17.8MB)\n",
            "\u001b[K     |████████████████████████████████| 17.8MB 10.6MB/s \n",
            "\u001b[?25hCollecting torch-model-archiver\n",
            "  Downloading https://files.pythonhosted.org/packages/00/16/214696ae401f308d1353466674b153769536ecaf7f5c64539df3972ecdab/torch_model_archiver-0.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from torchserve) (5.4.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from torchserve) (7.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torchserve) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchserve) (20.9)\n",
            "Collecting enum-compat\n",
            "  Downloading https://files.pythonhosted.org/packages/55/ae/467bc4509246283bb59746e21a1a2f5a8aecbef56b1fa6eaca78cd438c8b/enum_compat-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchserve) (2.4.7)\n",
            "Installing collected packages: torchserve, enum-compat, torch-model-archiver\n",
            "Successfully installed enum-compat-0.0.3 torch-model-archiver-0.3.1 torchserve-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xoth08XwMCTt"
      },
      "source": [
        "!torch-model-archiver --model-name pcam_cnn_v1.2 \\\n",
        "                     --version 1.2 \\\n",
        "                     --serialized-file pcam_cnn_v1.2.pt \\\n",
        "                     --handler pcam_classifier_handler.py"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TndNuHUwNZzq",
        "outputId": "06511106-5e67-46c0-ab95-a4157330ab81"
      },
      "source": [
        "!curl --netrc-file login_stuff https://model-store.scancer.org/models/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter host password for user 'mlops':\n",
            "{\n",
            "  \"models\": [\n",
            "    {\n",
            "      \"modelName\": \"pcam_cnn\",\n",
            "      \"modelUrl\": \"pcam-classification.mar\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXaXWU8CNwEt",
        "outputId": "9733cbc1-47c8-494c-a511-17e1e3352a8a"
      },
      "source": [
        "!curl -u mlops https://model-store.scancer.org/models/pcam_cnn_v1.1\n",
        "\n",
        "# url stored model\n",
        "# https://github.com/scancer-org/ml-pcam-classification/raw/main/models/pcam_cnn_v1.1.mar\n",
        "# raw file https://github.com/scancer-org/ml-pcam-classification/blob/main/models/pcam_cnn_v1.1.mar?raw=true"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter host password for user 'mlops':\n",
            "[\n",
            "  {\n",
            "    \"modelName\": \"pcam_cnn_v1.1\",\n",
            "    \"modelVersion\": \"1.1\",\n",
            "    \"modelUrl\": \"https://github.com/scancer-org/ml-pcam-classification/blob/main/models/pcam_cnn_v1.1.mar?raw\\u003dtrue\",\n",
            "    \"runtime\": \"python\",\n",
            "    \"minWorkers\": 0,\n",
            "    \"maxWorkers\": 0,\n",
            "    \"batchSize\": 1,\n",
            "    \"maxBatchDelay\": 100,\n",
            "    \"loadedAtStartup\": false,\n",
            "    \"workers\": []\n",
            "  }\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xZyX-K8Tnwr",
        "outputId": "0217ca25-0b3f-46a3-a021-00ec7fedf858"
      },
      "source": [
        "# !curl -X DELETE -u mlops https://model-store.scancer.org/models/pcam_cnn"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter host password for user 'mlops':\n",
            "{\n",
            "  \"status\": \"Model \\\"pcam_cnn\\\" unregistered\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0QW-K43QpZR",
        "outputId": "713b4523-ffb3-44e0-e89e-38ff4c1d66f3"
      },
      "source": [
        "!curl -X POST -u mlops https://model-store.scancer.org/models/?url=https://github.com/scancer-org/ml-pcam-classification/raw/main/models/pcam_cnn_v1.1.mar"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter host password for user 'mlops':\n",
            "{\n",
            "  \"code\": 409,\n",
            "  \"type\": \"ConflictStatusException\",\n",
            "  \"message\": \"Model version 1.1 is already registered for model pcam_cnn_v1.1\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXOuGjVdQ1_B",
        "outputId": "30ceb81b-4687-4ff4-8cc2-daafecc0a1a6"
      },
      "source": [
        "# !curl -X PUT -u mlops https://model-store.scancer.org/models/pcam_cnn_v1.1?min_worker=2&max_worker=2"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter host password for user 'mlops':\n",
            "<html>\n",
            "<head><title>401 Authorization Required</title></head>\n",
            "<body>\n",
            "<center><h1>401 Authorization Required</h1></center>\n",
            "<hr><center>nginx/1.18.0 (Ubuntu)</center>\n",
            "</body>\n",
            "</html>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-9NY96PVLRl",
        "outputId": "009f7d03-db40-402d-ade0-17168e265fb0"
      },
      "source": [
        "!curl -X PUT --netrc-file login_stuff https://model-store.scancer.org/models/pcam_cnn_v1.1?min_worker=2&max_worker=2"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"status\": \"Processing worker updates...\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyspyJluWY4B",
        "outputId": "e90bfe3b-2b51-45d2-ef05-0c7df7bd40e7"
      },
      "source": [
        "!curl --netrc-file login_stuff https://model-store.scancer.org/models/pcam_cnn_v1.1"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"modelName\": \"pcam_cnn_v1.1\",\n",
            "    \"modelVersion\": \"1.1\",\n",
            "    \"modelUrl\": \"https://github.com/scancer-org/ml-pcam-classification/blob/main/models/pcam_cnn_v1.1.mar?raw\\u003dtrue\",\n",
            "    \"runtime\": \"python\",\n",
            "    \"minWorkers\": 2,\n",
            "    \"maxWorkers\": 2,\n",
            "    \"batchSize\": 1,\n",
            "    \"maxBatchDelay\": 100,\n",
            "    \"loadedAtStartup\": false,\n",
            "    \"workers\": [\n",
            "      {\n",
            "        \"id\": \"9002\",\n",
            "        \"startTime\": \"2021-05-01T14:34:51.796Z\",\n",
            "        \"status\": \"READY\",\n",
            "        \"memoryUsage\": 0,\n",
            "        \"pid\": 404135,\n",
            "        \"gpu\": false,\n",
            "        \"gpuUsage\": \"N/A\"\n",
            "      },\n",
            "      {\n",
            "        \"id\": \"9003\",\n",
            "        \"startTime\": \"2021-05-01T14:34:51.804Z\",\n",
            "        \"status\": \"READY\",\n",
            "        \"memoryUsage\": 0,\n",
            "        \"pid\": 404136,\n",
            "        \"gpu\": false,\n",
            "        \"gpuUsage\": \"N/A\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq6JK4-aNm-6"
      },
      "source": [
        "username: mlops\n",
        "password: pE@u='i3>XVqBb{wNQcfHb$q@tHNgf"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}